{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecad8ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/21 10:18:54 WARN Utils: Your hostname, thorsten-pc, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/09/21 10:18:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/21 10:18:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Project3</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9f2d326cc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Project3\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"localhost\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a0c06",
   "metadata": {},
   "source": [
    "# Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c72e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rider_id: integer (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- birthday: date (nullable = true)\n",
      " |-- account_start_date: date (nullable = true)\n",
      " |-- account_end_date: date (nullable = true)\n",
      " |-- is_member: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType, BooleanType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"rider_id\", IntegerType(), False),\n",
    "    StructField(\"first\", StringType(), True),\n",
    "    StructField(\"last\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"birthday\", DateType(), True),\n",
    "    StructField(\"account_start_date\", DateType(), True),\n",
    "    StructField(\"account_end_date\", DateType(), True),\n",
    "    StructField(\"is_member\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "bronze_riders = spark.read.csv(\n",
    "    'data/riders.csv', \n",
    "    schema=schema\n",
    ")\n",
    "bronze_riders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9c3df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+--------+\n",
      "|payment_id|      date|amount|rider_id|\n",
      "+----------+----------+------+--------+\n",
      "|         1|2019-05-01|   9.0|    1000|\n",
      "|         2|2019-06-01|   9.0|    1000|\n",
      "|         3|2019-07-01|   9.0|    1000|\n",
      "|         4|2019-08-01|   9.0|    1000|\n",
      "|         5|2019-09-01|   9.0|    1000|\n",
      "+----------+----------+------+--------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- payment_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- rider_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"payment_id\", IntegerType(), False),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"rider_id\", IntegerType(), True)\n",
    "])\n",
    "bronze_payments = spark.read.csv(\n",
    "    'data/payments.csv', \n",
    "    schema=schema\n",
    ")\n",
    "bronze_payments.show(5)\n",
    "bronze_payments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bcff9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------------+------------------+\n",
      "|  station_id|                name|         latitude|         longitude|\n",
      "+------------+--------------------+-----------------+------------------+\n",
      "|         525|Glenwood Ave & To...|        42.012701|-87.66605799999999|\n",
      "|KA1503000012|  Clark St & Lake St|41.88579466666667|-87.63110066666668|\n",
      "|         637|Wood St & Chicago...|        41.895634|        -87.672069|\n",
      "|       13216|  State St & 33rd St|       41.8347335|       -87.6258275|\n",
      "|       18003|Fairbanks St & Su...|41.89580766666667|-87.62025316666669|\n",
      "+------------+--------------------+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- station_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"station_id\", StringType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"latitude\", DoubleType(), False),\n",
    "    StructField(\"longitude\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "bronze_stations = spark.read.csv(\n",
    "    'data/stations.csv', \n",
    "    schema=schema\n",
    ")\n",
    "bronze_stations.show(5)\n",
    "bronze_stations.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "675b6dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n",
      "|         trip_id|rideable_type|           start_at|           ended_at|start_station_id|end_station_id|rider_id|\n",
      "+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n",
      "|89E7AA6C29227EFF| classic_bike|2021-02-12 16:14:56|2021-02-12 16:21:43|             525|           660|   71934|\n",
      "|0FEFDE2603568365| classic_bike|2021-02-14 17:52:38|2021-02-14 18:12:09|             525|         16806|   47854|\n",
      "|E6159D746B2DBB91|electric_bike|2021-02-09 19:10:18|2021-02-09 19:19:10|    KA1503000012|  TA1305000029|   70870|\n",
      "|B32D3199F1C2E75B| classic_bike|2021-02-02 17:49:41|2021-02-02 17:54:06|             637|  TA1305000034|   58974|\n",
      "|83E463F23575F4BF|electric_bike|2021-02-23 15:07:23|2021-02-23 15:22:37|           13216|  TA1309000055|   39608|\n",
      "+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- rideable_type: string (nullable = true)\n",
      " |-- start_at: timestamp (nullable = true)\n",
      " |-- ended_at: timestamp (nullable = true)\n",
      " |-- start_station_id: string (nullable = true)\n",
      " |-- end_station_id: string (nullable = true)\n",
      " |-- rider_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"trip_id\", StringType(), False),\n",
    "    StructField(\"rideable_type\", StringType(), True),\n",
    "    StructField(\"start_at\", TimestampType(), True),\n",
    "    StructField(\"ended_at\", TimestampType(), True),\n",
    "    StructField(\"start_station_id\", StringType(), True),\n",
    "    StructField(\"end_station_id\", StringType(), True),\n",
    "    StructField(\"rider_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "bronze_trips = spark.read.csv(\n",
    "    'data/trips.csv',\n",
    "    schema=schema\n",
    ")\n",
    "bronze_trips.show(5)\n",
    "bronze_trips.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69dce77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bronze_payments.write.mode(\"overwrite\").format('csv').save('datalake/bronze_payments')\n",
    "bronze_stations.write.mode(\"overwrite\").format('csv').save('datalake/bronze_stations')\n",
    "bronze_riders.write.mode(\"overwrite\").format('csv').save('datalake/bronze_riders')\n",
    "bronze_trips.write.mode(\"overwrite\").format('csv').save('datalake/bronze_trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4255d288",
   "metadata": {},
   "source": [
    "# Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3521eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_riders.createOrReplaceTempView(\"Bronze_Riders\")\n",
    "bronze_payments.createOrReplaceTempView(\"Bronze_Payments\")\n",
    "bronze_stations.createOrReplaceTempView(\"Bronze_Stations\")\n",
    "bronze_trips.createOrReplaceTempView(\"Bronze_Trips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6a382",
   "metadata": {},
   "source": [
    "### Dimension Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd0ec2ae",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sql/dim_station.sql\n"
     ]
    }
   ],
   "source": [
    "%%file sql/dim_station.sql\n",
    "\n",
    "SELECT \n",
    "    station_id,\n",
    "    name,\n",
    "    latitude,\n",
    "    longitude\n",
    "FROM Bronze_Stations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfdb4dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------------+------------------+\n",
      "|  station_id|                name|         latitude|         longitude|\n",
      "+------------+--------------------+-----------------+------------------+\n",
      "|         525|Glenwood Ave & To...|        42.012701|-87.66605799999999|\n",
      "|KA1503000012|  Clark St & Lake St|41.88579466666667|-87.63110066666668|\n",
      "|         637|Wood St & Chicago...|        41.895634|        -87.672069|\n",
      "|       13216|  State St & 33rd St|       41.8347335|       -87.6258275|\n",
      "|       18003|Fairbanks St & Su...|41.89580766666667|-87.62025316666669|\n",
      "+------------+--------------------+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- station_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[station_id: string, name: string, latitude: double, longitude: double]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('sql/dim_station.sql', 'r') as f:\n",
    "    gold_dim_station = spark.sql(f.read())\n",
    "\n",
    "gold_dim_station.show(5)\n",
    "gold_dim_station.printSchema()\n",
    "gold_dim_station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67db2ff0",
   "metadata": {},
   "source": [
    "### Dimension Rider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb4c4ba5",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sql/dim_rider.sql\n"
     ]
    }
   ],
   "source": [
    "%%file sql/dim_rider.sql\n",
    "\n",
    "SELECT \n",
    "    rider_id, \n",
    "    address,\n",
    "    first as first_name,\n",
    "    last as last_name,\n",
    "    birthday,\n",
    "    is_member,\n",
    "    account_start_date,\n",
    "    account_end_date\n",
    "FROM Bronze_Riders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95d94ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+---------+----------+---------+------------------+----------------+\n",
      "|rider_id|             address|first_name|last_name|  birthday|is_member|account_start_date|account_end_date|\n",
      "+--------+--------------------+----------+---------+----------+---------+------------------+----------------+\n",
      "|    1000| 1200 Alyssa Squares|     Diana|    Clark|1989-02-13|     true|        2019-04-23|            NULL|\n",
      "|    1001|     397 Diana Ferry|  Jennifer|    Smith|1976-08-10|     true|        2019-11-01|      2020-09-01|\n",
      "|    1002|644 Brittany Row ...|     Karen|    Smith|1998-08-10|     true|        2022-02-04|            NULL|\n",
      "|    1003|996 Dickerson Tur...|     Bryan|  Roberts|1999-03-29|    false|        2019-08-26|            NULL|\n",
      "|    1004|7009 Nathan Expre...|     Jesse|Middleton|1969-04-11|     true|        2019-09-14|            NULL|\n",
      "+--------+--------------------+----------+---------+----------+---------+------------------+----------------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- rider_id: integer (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- birthday: date (nullable = true)\n",
      " |-- is_member: boolean (nullable = true)\n",
      " |-- account_start_date: date (nullable = true)\n",
      " |-- account_end_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('sql/dim_rider.sql', 'r') as f:\n",
    "    gold_dim_rider = spark.sql(f.read())\n",
    "\n",
    "gold_dim_rider.show(5)\n",
    "gold_dim_rider.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc243e5",
   "metadata": {},
   "source": [
    "### Dimension Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "036e7da3",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sql/dim_time.sql\n"
     ]
    }
   ],
   "source": [
    "%%file sql/dim_time.sql\n",
    "\n",
    "SELECT DISTINCT\n",
    "    date_format(a_timestamp, 'HHmmss') as time_string,\n",
    "    CAST(a_timestamp AS timestamp) as time,\n",
    "    hour(a_timestamp) as hour,\n",
    "    minute(a_timestamp) as minute,\n",
    "    second(a_timestamp) as second\n",
    "FROM (\n",
    "    SELECT start_at as a_timestamp FROM Bronze_Trips\n",
    "    UNION ALL\n",
    "    SELECT ended_at as a_timestamp FROM Bronze_Trips\n",
    ") tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de925e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:======================================>                 (44 + 20) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+----+------+------+\n",
      "|time_string|               time|hour|minute|second|\n",
      "+-----------+-------------------+----+------+------+\n",
      "|     205002|2021-02-23 20:50:02|  20|    50|     2|\n",
      "|     154934|2021-02-26 15:49:34|  15|    49|    34|\n",
      "|     204034|2021-02-03 20:40:34|  20|    40|    34|\n",
      "|     155241|2021-02-26 15:52:41|  15|    52|    41|\n",
      "|     165447|2021-02-28 16:54:47|  16|    54|    47|\n",
      "+-----------+-------------------+----+------+------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- time_string: string (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      " |-- second: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "with open('sql/dim_time.sql', 'r') as f:\n",
    "    gold_dim_time = spark.sql(f.read())\n",
    "\n",
    "gold_dim_time.show(5)\n",
    "gold_dim_time.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e27bd8c",
   "metadata": {},
   "source": [
    "### Dimension Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "378a5ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.last_day_of_quarter(dt)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType\n",
    "import datetime\n",
    "\n",
    "def last_day_of_quarter(dt):\n",
    "    if dt is None:\n",
    "        return None\n",
    "    q_month = ((dt.month - 1) // 3 + 1) * 3\n",
    "    if q_month == 12:\n",
    "        return datetime.date(dt.year, 12, 31)\n",
    "    next_month_first = datetime.date(dt.year, q_month + 1, 1)\n",
    "    return next_month_first - datetime.timedelta(days=1)\n",
    "\n",
    "spark.udf.register(\"last_day_of_quarter_udf\", last_day_of_quarter, DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d3e164f",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sql/dim_date.sql\n"
     ]
    }
   ],
   "source": [
    "%%file sql/dim_date.sql\n",
    "\n",
    "SELECT DISTINCT\n",
    "    date_format(a_date, 'yyyyMMdd') AS date_string,\n",
    "    CAST(a_date AS date) AS date,\n",
    "    year(a_date) AS year,\n",
    "    quarter(a_date) AS quarter,\n",
    "    month(a_date) AS month,\n",
    "    weekofyear(a_date) AS week,\n",
    "    day(a_date) AS day,\n",
    "    dayofweek(a_date) AS weekday,\n",
    "    date_format(a_date, 'EEEE') AS weekday_name,\n",
    "    date_format(a_date, 'MMMM') AS month_name,\n",
    "    trunc(a_date, 'YEAR') AS first_of_year,\n",
    "    date_add(trunc(a_date, 'YEAR'), CASE \n",
    "        WHEN ((year(a_date) % 4 = 0 AND year(a_date) % 100 <> 0) OR year(a_date) % 400 = 0) THEN 366\n",
    "        ELSE 365\n",
    "    END - 1) AS last_of_year,\n",
    "    trunc(a_date, 'QUARTER') AS first_of_quarter,\n",
    "    last_day_of_quarter_udf(a_date) AS last_of_quarter,\n",
    "    trunc(a_date, 'MONTH') AS first_of_month,\n",
    "    last_day(a_date) AS last_of_month,\n",
    "    ((year(a_date) % 4 = 0 AND year(a_date) % 100 <> 0) OR year(a_date) % 400 = 0) AS is_leap_year,\n",
    "    (dayofweek(a_date) IN (1,7)) AS is_weekend\n",
    "FROM (\n",
    "    SELECT start_at AS a_date FROM Bronze_Trips\n",
    "    UNION ALL\n",
    "    SELECT ended_at AS a_date FROM Bronze_Trips\n",
    "    UNION ALL\n",
    "    SELECT date AS a_date FROM Bronze_Payments\n",
    ") tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c72eba0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=============================================>          (64 + 14) / 78]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----+-------+-----+----+---+-------+------------+----------+-------------+------------+----------------+---------------+--------------+-------------+------------+----------+\n",
      "|date_string|      date|year|quarter|month|week|day|weekday|weekday_name|month_name|first_of_year|last_of_year|first_of_quarter|last_of_quarter|first_of_month|last_of_month|is_leap_year|is_weekend|\n",
      "+-----------+----------+----+-------+-----+----+---+-------+------------+----------+-------------+------------+----------------+---------------+--------------+-------------+------------+----------+\n",
      "|   20210315|2021-03-15|2021|      1|    3|  11| 15|      2|      Monday|     March|   2021-01-01|  2021-12-31|      2021-01-01|     2021-03-31|    2021-03-01|   2021-03-31|       false|     false|\n",
      "|   20210331|2021-03-31|2021|      1|    3|  13| 31|      4|   Wednesday|     March|   2021-01-01|  2021-12-31|      2021-01-01|     2021-03-31|    2021-03-01|   2021-03-31|       false|     false|\n",
      "|   20210226|2021-02-26|2021|      1|    2|   8| 26|      6|      Friday|  February|   2021-01-01|  2021-12-31|      2021-01-01|     2021-03-31|    2021-02-01|   2021-02-28|       false|     false|\n",
      "|   20210322|2021-03-22|2021|      1|    3|  12| 22|      2|      Monday|     March|   2021-01-01|  2021-12-31|      2021-01-01|     2021-03-31|    2021-03-01|   2021-03-31|       false|     false|\n",
      "|   20210326|2021-03-26|2021|      1|    3|  12| 26|      6|      Friday|     March|   2021-01-01|  2021-12-31|      2021-01-01|     2021-03-31|    2021-03-01|   2021-03-31|       false|     false|\n",
      "+-----------+----------+----+-------+-----+----+---+-------+------------+----------+-------------+------------+----------------+---------------+--------------+-------------+------------+----------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- date_string: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- weekday_name: string (nullable = true)\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- first_of_year: date (nullable = true)\n",
      " |-- last_of_year: date (nullable = true)\n",
      " |-- first_of_quarter: date (nullable = true)\n",
      " |-- last_of_quarter: date (nullable = true)\n",
      " |-- first_of_month: date (nullable = true)\n",
      " |-- last_of_month: date (nullable = true)\n",
      " |-- is_leap_year: boolean (nullable = true)\n",
      " |-- is_weekend: boolean (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "with open('sql/dim_date.sql', 'r') as f:\n",
    "    gold_dim_date = spark.sql(f.read())\n",
    "\n",
    "gold_dim_date.show(5)\n",
    "gold_dim_date.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1f38a",
   "metadata": {},
   "source": [
    "### Fact Payment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6677a945",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sql/fact_payment.sql\n"
     ]
    }
   ],
   "source": [
    "%%file sql/fact_payment.sql\n",
    "\n",
    "SELECT \n",
    "    payment_id,\n",
    "    to_char(date, 'yyyyMMdd') as payment_date,\n",
    "    rider_id,\n",
    "    amount\n",
    "FROM Bronze_Payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "694284e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+------+\n",
      "|payment_id|payment_date|rider_id|amount|\n",
      "+----------+------------+--------+------+\n",
      "|         1|    20190501|    1000|   9.0|\n",
      "|         2|    20190601|    1000|   9.0|\n",
      "|         3|    20190701|    1000|   9.0|\n",
      "|         4|    20190801|    1000|   9.0|\n",
      "|         5|    20190901|    1000|   9.0|\n",
      "+----------+------------+--------+------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- payment_id: integer (nullable = true)\n",
      " |-- payment_date: string (nullable = true)\n",
      " |-- rider_id: integer (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('sql/fact_payment.sql', 'r') as f:\n",
    "    gold_fact_payment = spark.sql(f.read())\n",
    "\n",
    "gold_fact_payment.show(5)\n",
    "gold_fact_payment.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ecd12",
   "metadata": {},
   "source": [
    "### Fact Trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad7ef6e7",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sql/fact_trip.sql\n"
     ]
    }
   ],
   "source": [
    "%%file sql/fact_trip.sql\n",
    "\n",
    "SELECT\n",
    "    bt.trip_id,\n",
    "    br.rider_id,\n",
    "    bt.start_station_id,\n",
    "    bt.end_station_id,\n",
    "    date_format(bt.start_at, 'yyyyMMdd') AS start_date,\n",
    "    date_format(bt.ended_at, 'yyyyMMdd') AS end_date,\n",
    "    date_format(bt.start_at, 'HHmmss') AS start_time,\n",
    "    date_format(bt.ended_at, 'HHmmss') AS end_time,\n",
    "    bt.rideable_type,\n",
    "    (unix_timestamp(bt.ended_at) - unix_timestamp(bt.start_at)) AS trip_duration_seconds,\n",
    "    floor(months_between(bt.ended_at, br.birthday) / 12) AS rider_age\n",
    "FROM Bronze_Trips bt\n",
    "JOIN Bronze_Riders br\n",
    "    ON bt.rider_id = br.rider_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31ee13d5",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+----------------+--------------+----------+--------+----------+--------+-------------+---------------------+---------+\n",
      "|         trip_id|rider_id|start_station_id|end_station_id|start_date|end_date|start_time|end_time|rideable_type|trip_duration_seconds|rider_age|\n",
      "+----------------+--------+----------------+--------------+----------+--------+----------+--------+-------------+---------------------+---------+\n",
      "|89E7AA6C29227EFF|   71934|             525|           660|  20210212|20210212|    161456|  162143| classic_bike|                  407|       37|\n",
      "|0FEFDE2603568365|   47854|             525|         16806|  20210214|20210214|    175238|  181209| classic_bike|                 1171|       38|\n",
      "|E6159D746B2DBB91|   70870|    KA1503000012|  TA1305000029|  20210209|20210209|    191018|  191910|electric_bike|                  532|       33|\n",
      "|B32D3199F1C2E75B|   58974|             637|  TA1305000034|  20210202|20210202|    174941|  175406| classic_bike|                  265|       19|\n",
      "|83E463F23575F4BF|   39608|           13216|  TA1309000055|  20210223|20210223|    150723|  152237|electric_bike|                  914|       71|\n",
      "+----------------+--------+----------------+--------------+----------+--------+----------+--------+-------------+---------------------+---------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- rider_id: integer (nullable = true)\n",
      " |-- start_station_id: string (nullable = true)\n",
      " |-- end_station_id: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- start_time: string (nullable = true)\n",
      " |-- end_time: string (nullable = true)\n",
      " |-- rideable_type: string (nullable = true)\n",
      " |-- trip_duration_seconds: long (nullable = true)\n",
      " |-- rider_age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('sql/fact_trip.sql', 'r') as f:\n",
    "    gold_fact_trip = spark.sql(f.read())\n",
    "\n",
    "gold_fact_trip.show(5)\n",
    "gold_fact_trip.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf4c6a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "gold_dim_station.write.mode(\"overwrite\").format('csv').save('datalake/gold_dim_station')\n",
    "gold_dim_rider.write.mode(\"overwrite\").format('csv').save('datalake/gold_dim_rider')\n",
    "gold_dim_time.write.mode(\"overwrite\").format('csv').save('datalake/gold_dim_time')\n",
    "gold_dim_date.write.mode(\"overwrite\").format('csv').save('datalake/gold_dim_date')\n",
    "gold_fact_payment.write.mode(\"overwrite\").format('csv').save('datalake/gold_fact_payment')\n",
    "gold_fact_trip.write.mode(\"overwrite\").format('csv').save('datalake/gold_fact_trip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6efd4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nd0277-data-engineering-with-microsoft-azure-project-3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
