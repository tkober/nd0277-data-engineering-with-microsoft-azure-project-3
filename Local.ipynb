{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecad8ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/21 09:59:53 WARN Utils: Your hostname, thorsten-pc, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/09/21 09:59:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/21 09:59:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Project3</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f326052ecc0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Project3\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"localhost\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a0c06",
   "metadata": {},
   "source": [
    "# Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c72e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rider_id: integer (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- birthday: date (nullable = true)\n",
      " |-- account_start_date: date (nullable = true)\n",
      " |-- account_end_date: date (nullable = true)\n",
      " |-- is_member: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType, BooleanType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"rider_id\", IntegerType(), False),\n",
    "    StructField(\"first\", StringType(), True),\n",
    "    StructField(\"last\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"birthday\", DateType(), True),\n",
    "    StructField(\"account_start_date\", DateType(), True),\n",
    "    StructField(\"account_end_date\", DateType(), True),\n",
    "    StructField(\"is_member\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "bronze_riders = spark.read.csv(\n",
    "    'data/riders.csv', \n",
    "    schema=schema\n",
    ")\n",
    "bronze_riders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9c3df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+--------+\n",
      "|payment_id|      date|amount|rider_id|\n",
      "+----------+----------+------+--------+\n",
      "|         1|2019-05-01|   9.0|    1000|\n",
      "|         2|2019-06-01|   9.0|    1000|\n",
      "|         3|2019-07-01|   9.0|    1000|\n",
      "|         4|2019-08-01|   9.0|    1000|\n",
      "|         5|2019-09-01|   9.0|    1000|\n",
      "+----------+----------+------+--------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- payment_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- rider_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"payment_id\", IntegerType(), False),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"rider_id\", IntegerType(), True)\n",
    "])\n",
    "bronze_payments = spark.read.csv(\n",
    "    'data/payments.csv', \n",
    "    schema=schema\n",
    ")\n",
    "bronze_payments.show(5)\n",
    "bronze_payments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bcff9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------------+------------------+\n",
      "|  station_id|                name|         latitude|         longitude|\n",
      "+------------+--------------------+-----------------+------------------+\n",
      "|         525|Glenwood Ave & To...|        42.012701|-87.66605799999999|\n",
      "|KA1503000012|  Clark St & Lake St|41.88579466666667|-87.63110066666668|\n",
      "|         637|Wood St & Chicago...|        41.895634|        -87.672069|\n",
      "|       13216|  State St & 33rd St|       41.8347335|       -87.6258275|\n",
      "|       18003|Fairbanks St & Su...|41.89580766666667|-87.62025316666669|\n",
      "+------------+--------------------+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- station_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"station_id\", StringType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"latitude\", DoubleType(), False),\n",
    "    StructField(\"longitude\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "bronze_stations = spark.read.csv(\n",
    "    'data/stations.csv', \n",
    "    schema=schema\n",
    ")\n",
    "bronze_stations.show(5)\n",
    "bronze_stations.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "675b6dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n",
      "|         trip_id|rideable_type|           start_at|           ended_at|start_station_id|end_station_id|rider_id|\n",
      "+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n",
      "|89E7AA6C29227EFF| classic_bike|2021-02-12 16:14:56|2021-02-12 16:21:43|             525|           660|   71934|\n",
      "|0FEFDE2603568365| classic_bike|2021-02-14 17:52:38|2021-02-14 18:12:09|             525|         16806|   47854|\n",
      "|E6159D746B2DBB91|electric_bike|2021-02-09 19:10:18|2021-02-09 19:19:10|    KA1503000012|  TA1305000029|   70870|\n",
      "|B32D3199F1C2E75B| classic_bike|2021-02-02 17:49:41|2021-02-02 17:54:06|             637|  TA1305000034|   58974|\n",
      "|83E463F23575F4BF|electric_bike|2021-02-23 15:07:23|2021-02-23 15:22:37|           13216|  TA1309000055|   39608|\n",
      "+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- rideable_type: string (nullable = true)\n",
      " |-- start_at: timestamp (nullable = true)\n",
      " |-- ended_at: timestamp (nullable = true)\n",
      " |-- start_station_id: string (nullable = true)\n",
      " |-- end_station_id: string (nullable = true)\n",
      " |-- rider_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"trip_id\", StringType(), False),\n",
    "    StructField(\"rideable_type\", StringType(), True),\n",
    "    StructField(\"start_at\", TimestampType(), True),\n",
    "    StructField(\"ended_at\", TimestampType(), True),\n",
    "    StructField(\"start_station_id\", StringType(), True),\n",
    "    StructField(\"end_station_id\", StringType(), True),\n",
    "    StructField(\"rider_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "bronze_trips = spark.read.csv(\n",
    "    'data/trips.csv',\n",
    "    schema=schema\n",
    ")\n",
    "bronze_trips.show(5)\n",
    "bronze_trips.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4255d288",
   "metadata": {},
   "source": [
    "# Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3521eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_riders.createOrReplaceTempView(\"Bronze_Riders\")\n",
    "bronze_payments.createOrReplaceTempView(\"Bronze_Payments\")\n",
    "bronze_stations.createOrReplaceTempView(\"Bronze_Stations\")\n",
    "bronze_trips.createOrReplaceTempView(\"Bronze_Trips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6a382",
   "metadata": {},
   "source": [
    "### Dimension Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd0ec2ae",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sql/dim_station.sql\n"
     ]
    }
   ],
   "source": [
    "%%file sql/dim_station.sql\n",
    "\n",
    "SELECT \n",
    "    station_id,\n",
    "    name,\n",
    "    latitude,\n",
    "    longitude\n",
    "FROM Bronze_Stations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfdb4dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------------+------------------+\n",
      "|  station_id|                name|         latitude|         longitude|\n",
      "+------------+--------------------+-----------------+------------------+\n",
      "|         525|Glenwood Ave & To...|        42.012701|-87.66605799999999|\n",
      "|KA1503000012|  Clark St & Lake St|41.88579466666667|-87.63110066666668|\n",
      "|         637|Wood St & Chicago...|        41.895634|        -87.672069|\n",
      "|       13216|  State St & 33rd St|       41.8347335|       -87.6258275|\n",
      "|       18003|Fairbanks St & Su...|41.89580766666667|-87.62025316666669|\n",
      "+------------+--------------------+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- station_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[station_id: string, name: string, latitude: double, longitude: double]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('sql/dim_station.sql', 'r') as f:\n",
    "    gold_dim_station = spark.sql(f.read())\n",
    "\n",
    "gold_dim_station.show(5)\n",
    "gold_dim_station.printSchema()\n",
    "gold_dim_station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67db2ff0",
   "metadata": {},
   "source": [
    "### Dimension Rider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb4c4ba5",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sql/dim_rider.sql\n"
     ]
    }
   ],
   "source": [
    "%%file sql/dim_rider.sql\n",
    "\n",
    "SELECT \n",
    "    rider_id, \n",
    "    address,\n",
    "    first as first_name,\n",
    "    last as last_name,\n",
    "    birthday,\n",
    "    is_member,\n",
    "    account_start_date,\n",
    "    account_end_date\n",
    "FROM Bronze_Riders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95d94ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+---------+----------+---------+------------------+----------------+\n",
      "|rider_id|             address|first_name|last_name|  birthday|is_member|account_start_date|account_end_date|\n",
      "+--------+--------------------+----------+---------+----------+---------+------------------+----------------+\n",
      "|    1000| 1200 Alyssa Squares|     Diana|    Clark|1989-02-13|     true|        2019-04-23|            NULL|\n",
      "|    1001|     397 Diana Ferry|  Jennifer|    Smith|1976-08-10|     true|        2019-11-01|      2020-09-01|\n",
      "|    1002|644 Brittany Row ...|     Karen|    Smith|1998-08-10|     true|        2022-02-04|            NULL|\n",
      "|    1003|996 Dickerson Tur...|     Bryan|  Roberts|1999-03-29|    false|        2019-08-26|            NULL|\n",
      "|    1004|7009 Nathan Expre...|     Jesse|Middleton|1969-04-11|     true|        2019-09-14|            NULL|\n",
      "+--------+--------------------+----------+---------+----------+---------+------------------+----------------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- rider_id: integer (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- birthday: date (nullable = true)\n",
      " |-- is_member: boolean (nullable = true)\n",
      " |-- account_start_date: date (nullable = true)\n",
      " |-- account_end_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('sql/dim_rider.sql', 'r') as f:\n",
    "    gold_dim_rider = spark.sql(f.read())\n",
    "\n",
    "gold_dim_rider.show(5)\n",
    "gold_dim_rider.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc243e5",
   "metadata": {},
   "source": [
    "### Dimension Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "036e7da3",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sql/dim_time.sql\n"
     ]
    }
   ],
   "source": [
    "%%file sql/dim_time.sql\n",
    "\n",
    "SELECT DISTINCT\n",
    "    date_format(a_timestamp, 'HHmmss') as time_string,\n",
    "    CAST(a_timestamp AS timestamp) as time,\n",
    "    hour(a_timestamp) as hour,\n",
    "    minute(a_timestamp) as minute,\n",
    "    second(a_timestamp) as second\n",
    "FROM (\n",
    "    SELECT start_at as a_timestamp FROM Bronze_Trips\n",
    "    UNION ALL\n",
    "    SELECT ended_at as a_timestamp FROM Bronze_Trips\n",
    ") tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de925e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:========================================>               (46 + 18) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+----+------+------+\n",
      "|time_string|               time|hour|minute|second|\n",
      "+-----------+-------------------+----+------+------+\n",
      "|     205002|2021-02-23 20:50:02|  20|    50|     2|\n",
      "|     154934|2021-02-26 15:49:34|  15|    49|    34|\n",
      "|     204034|2021-02-03 20:40:34|  20|    40|    34|\n",
      "|     155241|2021-02-26 15:52:41|  15|    52|    41|\n",
      "|     165447|2021-02-28 16:54:47|  16|    54|    47|\n",
      "+-----------+-------------------+----+------+------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- time_string: string (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      " |-- second: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "with open('sql/dim_time.sql', 'r') as f:\n",
    "    gold_dim_time = spark.sql(f.read())\n",
    "\n",
    "gold_dim_time.show(5)\n",
    "gold_dim_time.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e27bd8c",
   "metadata": {},
   "source": [
    "### Dimension Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "378a5ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.last_day_of_quarter(dt)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType\n",
    "import datetime\n",
    "\n",
    "def last_day_of_quarter(dt):\n",
    "    if dt is None:\n",
    "        return None\n",
    "    q_month = ((dt.month - 1) // 3 + 1) * 3\n",
    "    if q_month == 12:\n",
    "        return datetime.date(dt.year, 12, 31)\n",
    "    next_month_first = datetime.date(dt.year, q_month + 1, 1)\n",
    "    return next_month_first - datetime.timedelta(days=1)\n",
    "\n",
    "spark.udf.register(\"last_day_of_quarter_udf\", last_day_of_quarter, DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d3e164f",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sql/dim_date.sql\n"
     ]
    }
   ],
   "source": [
    "%%file sql/dim_date.sql\n",
    "\n",
    "SELECT DISTINCT\n",
    "    date_format(a_date, 'yyyyMMdd') AS date_string,\n",
    "    CAST(a_date AS date) AS date,\n",
    "    year(a_date) AS year,\n",
    "    quarter(a_date) AS quarter,\n",
    "    month(a_date) AS month,\n",
    "    weekofyear(a_date) AS week,\n",
    "    day(a_date) AS day,\n",
    "    dayofweek(a_date) AS weekday,\n",
    "    date_format(a_date, 'EEEE') AS weekday_name,\n",
    "    date_format(a_date, 'MMMM') AS month_name,\n",
    "    trunc(a_date, 'YEAR') AS first_of_year,\n",
    "    date_add(trunc(a_date, 'YEAR'), CASE \n",
    "        WHEN ((year(a_date) % 4 = 0 AND year(a_date) % 100 <> 0) OR year(a_date) % 400 = 0) THEN 366\n",
    "        ELSE 365\n",
    "    END - 1) AS last_of_year,\n",
    "    trunc(a_date, 'QUARTER') AS first_of_quarter,\n",
    "    last_day_of_quarter_udf(a_date) AS last_of_quarter,\n",
    "    trunc(a_date, 'MONTH') AS first_of_month,\n",
    "    last_day(a_date) AS last_of_month,\n",
    "    ((year(a_date) % 4 = 0 AND year(a_date) % 100 <> 0) OR year(a_date) % 400 = 0) AS is_leap_year,\n",
    "    (dayofweek(a_date) IN (1,7)) AS is_weekend\n",
    "FROM (\n",
    "    SELECT start_at AS a_date FROM Bronze_Trips\n",
    "    UNION ALL\n",
    "    SELECT ended_at AS a_date FROM Bronze_Trips\n",
    "    UNION ALL\n",
    "    SELECT date AS a_date FROM Bronze_Payments\n",
    ") tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c72eba0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=============================================>         (64 + 14) / 78]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----+-------+-----+----+---+-------+------------+----------+-------------+------------+----------------+---------------+--------------+-------------+------------+----------+\n",
      "|date_string|      date|year|quarter|month|week|day|weekday|weekday_name|month_name|first_of_year|last_of_year|first_of_quarter|last_of_quarter|first_of_month|last_of_month|is_leap_year|is_weekend|\n",
      "+-----------+----------+----+-------+-----+----+---+-------+------------+----------+-------------+------------+----------------+---------------+--------------+-------------+------------+----------+\n",
      "|   20210315|2021-03-15|2021|      1|    3|  11| 15|      2|      Monday|     March|   2021-01-01|  2021-12-31|      2021-01-01|     2021-03-31|    2021-03-01|   2021-03-31|       false|     false|\n",
      "|   20210331|2021-03-31|2021|      1|    3|  13| 31|      4|   Wednesday|     March|   2021-01-01|  2021-12-31|      2021-01-01|     2021-03-31|    2021-03-01|   2021-03-31|       false|     false|\n",
      "|   20210226|2021-02-26|2021|      1|    2|   8| 26|      6|      Friday|  February|   2021-01-01|  2021-12-31|      2021-01-01|     2021-03-31|    2021-02-01|   2021-02-28|       false|     false|\n",
      "|   20210322|2021-03-22|2021|      1|    3|  12| 22|      2|      Monday|     March|   2021-01-01|  2021-12-31|      2021-01-01|     2021-03-31|    2021-03-01|   2021-03-31|       false|     false|\n",
      "|   20210326|2021-03-26|2021|      1|    3|  12| 26|      6|      Friday|     March|   2021-01-01|  2021-12-31|      2021-01-01|     2021-03-31|    2021-03-01|   2021-03-31|       false|     false|\n",
      "+-----------+----------+----+-------+-----+----+---+-------+------------+----------+-------------+------------+----------------+---------------+--------------+-------------+------------+----------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- date_string: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      " |-- weekday_name: string (nullable = true)\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- first_of_year: date (nullable = true)\n",
      " |-- last_of_year: date (nullable = true)\n",
      " |-- first_of_quarter: date (nullable = true)\n",
      " |-- last_of_quarter: date (nullable = true)\n",
      " |-- first_of_month: date (nullable = true)\n",
      " |-- last_of_month: date (nullable = true)\n",
      " |-- is_leap_year: boolean (nullable = true)\n",
      " |-- is_weekend: boolean (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "with open('sql/dim_date.sql', 'r') as f:\n",
    "    gold_dim_date = spark.sql(f.read())\n",
    "\n",
    "gold_dim_date.show(5)\n",
    "gold_dim_date.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1f38a",
   "metadata": {},
   "source": [
    "### Fact Payment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6677a945",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sql/fact_payment.sql\n"
     ]
    }
   ],
   "source": [
    "%%file sql/fact_payment.sql\n",
    "\n",
    "SELECT \n",
    "    payment_id,\n",
    "    to_char(date, 'yyyyMMdd') as payment_date,\n",
    "    rider_id,\n",
    "    amount\n",
    "FROM Bronze_Payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "694284e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+------+\n",
      "|payment_id|payment_date|rider_id|amount|\n",
      "+----------+------------+--------+------+\n",
      "|         1|    20190501|    1000|   9.0|\n",
      "|         2|    20190601|    1000|   9.0|\n",
      "|         3|    20190701|    1000|   9.0|\n",
      "|         4|    20190801|    1000|   9.0|\n",
      "|         5|    20190901|    1000|   9.0|\n",
      "+----------+------------+--------+------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- payment_id: integer (nullable = true)\n",
      " |-- payment_date: string (nullable = true)\n",
      " |-- rider_id: integer (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('sql/fact_payment.sql', 'r') as f:\n",
    "    gold_fact_payment = spark.sql(f.read())\n",
    "\n",
    "gold_fact_payment.show(5)\n",
    "gold_fact_payment.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ecd12",
   "metadata": {},
   "source": [
    "### Fact Trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad7ef6e7",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sql/fact_trip.sql\n"
     ]
    }
   ],
   "source": [
    "%%file sql/fact_trip.sql\n",
    "\n",
    "SELECT\n",
    "    bt.trip_id,\n",
    "    br.rider_id,\n",
    "    bt.start_station_id,\n",
    "    bt.end_station_id,\n",
    "    date_format(bt.start_at, 'yyyyMMdd') AS start_date,\n",
    "    date_format(bt.ended_at, 'yyyyMMdd') AS end_date,\n",
    "    date_format(bt.start_at, 'HHmmss') AS start_time,\n",
    "    date_format(bt.ended_at, 'HHmmss') AS end_time,\n",
    "    bt.rideable_type,\n",
    "    (unix_timestamp(bt.ended_at) - unix_timestamp(bt.start_at)) AS trip_duration_seconds,\n",
    "    floor(months_between(bt.ended_at, br.birthday) / 12) AS rider_age\n",
    "FROM Bronze_Trips bt\n",
    "JOIN Bronze_Riders br\n",
    "    ON bt.rider_id = br.rider_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31ee13d5",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+----------------+--------------+----------+--------+----------+--------+-------------+---------------------+---------+\n",
      "|         trip_id|rider_id|start_station_id|end_station_id|start_date|end_date|start_time|end_time|rideable_type|trip_duration_seconds|rider_age|\n",
      "+----------------+--------+----------------+--------------+----------+--------+----------+--------+-------------+---------------------+---------+\n",
      "|89E7AA6C29227EFF|   71934|             525|           660|  20210212|20210212|    161456|  162143| classic_bike|                  407|       37|\n",
      "|0FEFDE2603568365|   47854|             525|         16806|  20210214|20210214|    175238|  181209| classic_bike|                 1171|       38|\n",
      "|E6159D746B2DBB91|   70870|    KA1503000012|  TA1305000029|  20210209|20210209|    191018|  191910|electric_bike|                  532|       33|\n",
      "|B32D3199F1C2E75B|   58974|             637|  TA1305000034|  20210202|20210202|    174941|  175406| classic_bike|                  265|       19|\n",
      "|83E463F23575F4BF|   39608|           13216|  TA1309000055|  20210223|20210223|    150723|  152237|electric_bike|                  914|       71|\n",
      "+----------------+--------+----------------+--------------+----------+--------+----------+--------+-------------+---------------------+---------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- rider_id: integer (nullable = true)\n",
      " |-- start_station_id: string (nullable = true)\n",
      " |-- end_station_id: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- start_time: string (nullable = true)\n",
      " |-- end_time: string (nullable = true)\n",
      " |-- rideable_type: string (nullable = true)\n",
      " |-- trip_duration_seconds: long (nullable = true)\n",
      " |-- rider_age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('sql/fact_trip.sql', 'r') as f:\n",
    "    gold_fact_trip = spark.sql(f.read())\n",
    "\n",
    "gold_fact_trip.show(5)\n",
    "gold_fact_trip.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf4c6a8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o164.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Make sure the provider name is correct and the package is properly registered and compatible with your Spark version. SQLSTATE: 42K02\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:681)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:740)\n\tat org.apache.spark.sql.classic.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:626)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:135)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$6(DataSource.scala:665)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:665)\n\tat scala.util.Failure.orElse(Try.scala:230)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:665)\n\t... 16 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgold_dim_station\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdelta\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdelta/gold_dim_station\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/nd0277-data-engineering-with-microsoft-azure-project-3/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:1745\u001b[39m, in \u001b[36mDataFrameWriter.save\u001b[39m\u001b[34m(self, path, format, mode, partitionBy, **options)\u001b[39m\n\u001b[32m   1743\u001b[39m     \u001b[38;5;28mself\u001b[39m._jwrite.save()\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1745\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/nd0277-data-engineering-with-microsoft-azure-project-3/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/nd0277-data-engineering-with-microsoft-azure-project-3/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/nd0277-data-engineering-with-microsoft-azure-project-3/.venv/lib/python3.12/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o164.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Make sure the provider name is correct and the package is properly registered and compatible with your Spark version. SQLSTATE: 42K02\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:681)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:740)\n\tat org.apache.spark.sql.classic.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:626)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:135)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$6(DataSource.scala:665)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:665)\n\tat scala.util.Failure.orElse(Try.scala:230)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:665)\n\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "gold_dim_station.write.format('delta').save('delta/gold_dim_station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6efd4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nd0277-data-engineering-with-microsoft-azure-project-3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
